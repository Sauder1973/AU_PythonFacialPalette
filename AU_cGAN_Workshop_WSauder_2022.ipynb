{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vi_fMPvuXllU",
        "2wKfnCZPZSZg",
        "1v17_kNz6OyK"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN+lawXpHzpMg+KsJb/1MTB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sauder1973/AU_PythonFacialPalette/blob/master/AU_cGAN_Workshop_WSauder_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Conditional Generative Adversarial Network Workshop (cGAN)**\n",
        "\n",
        "## Presented by Wes Sauder - MScIS Student - Athabasca University\n",
        "\n",
        "November 12, 2023\n",
        "\n",
        "Welcome to the workshop!!\n",
        "\n"
      ],
      "metadata": {
        "id": "vi_fMPvuXllU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Goals and Objectives:**\n",
        "\n",
        "This workshop will guide the learner through a process to develop an understanding of GANS.  During the course of this workshop, this Colab workbook will:\n",
        " \n",
        "*   Initial Environment Setup.\n",
        "*   Lesson I  : **Data Loader** for Training images  -  *Real Time*\n",
        "*   Lesson II : **Construct** a cGAN model  - *Real Time*\n",
        "*   Lesson III: **Train Model**  -  *For your trial after the workshop Approx 1 hour to run, example of code provided*\n",
        "*   Lesson IV: **Execute Model** - *Real Time with a 'saved' model*\n",
        "*   Lesson V  : Evaluate effect of **'input noise'** - *Real Time Experiments*\n",
        "*   Lesson VI: **Imputation** of models Latent Space - *Real Time Experiments*\n",
        "\n",
        "Each section is clearly evaluated in the code below, along with instructions how to access test data\n",
        "\n",
        "**The Goal** is to train a Conditional GAN or cGAN model for several different facial expressions.  \n",
        "\n",
        "Supplied to this notebook are **Four** facial expressions which were previously clustered based on their similiarity to each other and assigned a specific expression catgegory which include:\n",
        "\n",
        "\n",
        "\n",
        "1.   Neutral Face\n",
        "2.   Happy Face\n",
        "3.   Angry Face\n",
        "4.   Sad Face\n",
        "\n",
        "Once a model has been trained, the generated images from the cGAN can be recalled, altered by changing the input 'Noise' vector, as well as 'imputed' where a continuum of results is possible between two different categories."
      ],
      "metadata": {
        "id": "2wKfnCZPZSZg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Datasets**\n",
        "\n",
        "The data used in this workshop are images.  We may not think of images as 'data'  in the traditional tabular information models.  However, images are (x,y) plots of a variable data.  The images are captured frames from a larger set of video called the ['RAVDESS dataset'](https://zenodo.org/record/1188976) or The Ryerson Audio-Visual Database of Emotional Speech and Song \n",
        "\n",
        "***Reference:***\n",
        "*Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391.*\n",
        "\n",
        "***License information***\n",
        "*“The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)” by Livingstone & Russo is licensed under CC BY-NA-SC 4.0.*\n",
        "\n",
        "**Description**\n",
        "\n",
        "The dataset contains the complete set of 7356 RAVDESS files (total size: 24.8 GB). Each of the 24 actors consists of three modality formats: Audio-only (16bit, 48kHz .wav), Audio-Video (720p H.264, AAC 48kHz, .mp4), and Video-only (no sound). Note, there are no song files for Actor_18.\n",
        "\n",
        "\n",
        "\n",
        "**File naming convention**\n",
        "\n",
        "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.mp4). These identifiers define the stimulus characteristics:\n",
        "\n",
        "Filename identifiers\n",
        "\n",
        "Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "\n",
        "*   Vocal channel (01 = speech, 02 = song).\n",
        "*   Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "*   Emotional intensity (01 = normal, 02 = strong). **NOTE**: There is no strong intensity for the ‘neutral’ emotion.\n",
        "*   Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
        "*   Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "*   Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "**Filename example: 02-01-06-01-02-01-12.mp4**\n",
        "Video-only (02)\n",
        "Speech (01)\n",
        "Fearful (06)\n",
        "Normal intensity (01)\n",
        "Statement “dogs” (02)\n",
        "1st Repetition (01)\n",
        "12th Actor (12)\n",
        "Female, as the actor ID number is even.\n",
        "\n",
        "\n",
        "**Please Note!!**\n",
        "In this workshop, only **Actor 12** was used, and a subset of the facial expressions is used, where only the three clustered expressions have been provided in this analysis."
      ],
      "metadata": {
        "id": "1v17_kNz6OyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initial Environment Setup\n",
        "\n",
        "\n",
        "**Runtime Setup**\n",
        "\n",
        "At this time, please attempt the following:\n",
        "\n",
        "1.   Navigate to the top ribbon\n",
        "2.   Select **Runtime**\n",
        "3.   Select **Change Runtime Type**\n",
        "4.   Select **Hardware accelerator** and choose **GPU**\n",
        "5.   Select **Runtime Sahpe** and choose **High-Ram**\n",
        "\n",
        "Please note, unless you have a Colab Pro Account, you may have some issues selecting these options based on availability.  If you are unable to select these options, or get 'kicked' out of the session, please try again at a later time.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zq7dJngF3ien"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Packages and Libraries Installation**\n",
        "\n",
        "The following Python libraries are required to build the GANS, as well as visualize and process the code."
      ],
      "metadata": {
        "id": "McfhVtn8cxTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch Libary\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Torch Visualization\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms as T\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.transforms import Resize\n",
        "import torchvision\n",
        "\n",
        "# Torch Dataloader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Python Analysis Packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "#from matplotlib import pyplot as plt\n",
        "import random\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "import datetime\n",
        "import pytz\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "KvtpLe66cv09"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Functions**\n",
        "\n",
        "The following functions are required later in the code. "
      ],
      "metadata": {
        "id": "UJCBr1i3d1L6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
        "    '''\n",
        "    Function for visualizing images: Given a tensor of images, number of images, and\n",
        "    size per image, plots and prints the images in an uniform grid.\n",
        "    '''\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
        "    #axes = plt.gca()\n",
        "    #axes.set_aspect(.5)\n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "    if show:\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "ro6EfD1qXk_Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup Global Variables**"
      ],
      "metadata": {
        "id": "ig5Zu0yFeF86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the Seed\n",
        "torch.manual_seed(999) # Set for our testing purposes, please do not change!\n",
        "random.seed(999)\n",
        "\n",
        "# Set the Run_Training Variable\n",
        "\n",
        "Run_Training = False"
      ],
      "metadata": {
        "id": "Zoq5UVeoXlCD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CHAPTER I - Loading Data into Dataloader\n",
        "\n",
        "In order to conserve time, please connect to GitHub and pull the Image Data required to train the GAN.  While we discuss some basic theory, the data will be loaded into the file space for us to use later. \n",
        "\n",
        "**Get Data from GitHub and Save In Colab Session**\n",
        "\n",
        "To speed up the exercise, preclustered facial expressions have been cropped from video and stored on [GitHub](https://github.com/Sauder1973/faceImages).  Please note, you will attempt to load the data into the dataloader, but will unlikely use it during the training since the time to train a working GAN model exceeds the length of the workshop.  However, we will make sure that these steps still work properly in order for you to try this notebook after the workshop.\n"
      ],
      "metadata": {
        "id": "puw9SL-bxfYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Sauder1973/faceImages.git\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYtH3eeTx4Vp",
        "outputId": "859b3137-bae3-4183-c8d4-8df81dcfa588"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'faceImages' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once complete, please check the file directory to the left of the coding box under files.  You should see a new directory with the image files required for training.  \n",
        "**Please note:**  For this workshop, you may not be able to actually run the dataloader and perform the training in a single step.  This code will allow you to run trials after the workshop.  In the meantime, to ensure you have everything you need, continue to run all the sections until we reach ***Lesson III: Training***"
      ],
      "metadata": {
        "id": "5sckT68X2p8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fileName = '/content/faceImages/FaceClusters.csv'\n",
        "\n",
        "#Read CSV\n",
        "\n",
        "df_FaceClusters = pd.read_csv(fileName)\n",
        "\n",
        "df_FaceClusters\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9ThbkGZMFvT0",
        "outputId": "dd03a8b2-5b6d-45b3-f343-2bb3b127dd60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ClassLabel  FileNumber                 FileName EmotionLabel\n",
              "0              4        1136  Image_1136_FullFace.jpg        Angry\n",
              "1              4        1137  Image_1137_FullFace.jpg        Angry\n",
              "2              4        1138  Image_1138_FullFace.jpg        Angry\n",
              "3              4        1234  Image_1234_FullFace.jpg        Angry\n",
              "4              4        1323  Image_1323_FullFace.jpg        Angry\n",
              "...          ...         ...                      ...          ...\n",
              "1209          24        4623  Image_4623_FullFace.jpg        Happy\n",
              "1210          24        4628  Image_4628_FullFace.jpg        Happy\n",
              "1211          24        4714  Image_4714_FullFace.jpg        Happy\n",
              "1212          24        4715  Image_4715_FullFace.jpg        Happy\n",
              "1213          24        5187  Image_5187_FullFace.jpg        Happy\n",
              "\n",
              "[1214 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2127ea9-849e-444f-9074-74377eada40b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ClassLabel</th>\n",
              "      <th>FileNumber</th>\n",
              "      <th>FileName</th>\n",
              "      <th>EmotionLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>1136</td>\n",
              "      <td>Image_1136_FullFace.jpg</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>1137</td>\n",
              "      <td>Image_1137_FullFace.jpg</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>1138</td>\n",
              "      <td>Image_1138_FullFace.jpg</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1234</td>\n",
              "      <td>Image_1234_FullFace.jpg</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1323</td>\n",
              "      <td>Image_1323_FullFace.jpg</td>\n",
              "      <td>Angry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1209</th>\n",
              "      <td>24</td>\n",
              "      <td>4623</td>\n",
              "      <td>Image_4623_FullFace.jpg</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1210</th>\n",
              "      <td>24</td>\n",
              "      <td>4628</td>\n",
              "      <td>Image_4628_FullFace.jpg</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1211</th>\n",
              "      <td>24</td>\n",
              "      <td>4714</td>\n",
              "      <td>Image_4714_FullFace.jpg</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1212</th>\n",
              "      <td>24</td>\n",
              "      <td>4715</td>\n",
              "      <td>Image_4715_FullFace.jpg</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1213</th>\n",
              "      <td>24</td>\n",
              "      <td>5187</td>\n",
              "      <td>Image_5187_FullFace.jpg</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1214 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2127ea9-849e-444f-9074-74377eada40b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e2127ea9-849e-444f-9074-74377eada40b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e2127ea9-849e-444f-9074-74377eada40b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next Confirm the directory 'faceImages' exists and the Zip File can be found within it.  Execute the following in order to unzip the image files.  The next line will 'inflate' the zipped image folder placing the files in a new directory called 'FaceFiles'.  This directory will be used to pull the files for the dataloader.\n",
        "\n",
        "Please note, once the session times out or disconnects, these files will automatically be removed by Colab.  Therefore, you will need to run this process even if you have already done these steps before.  This drive is only temporarory, unlike a traditional 'Google Drive' account which will stay active and is non volatile."
      ],
      "metadata": {
        "id": "tPwwJh6wqkYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/faceImages/Image_0115_FullFace.zip -d FaceFiles"
      ],
      "metadata": {
        "id": "kexC7DntGI8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "phGbc4wDGJAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data into Memory using PyTorch Data Loader**\n",
        "\n",
        "Please refer to the following for information regarding the dataloader: [PyTorch Data Loader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
        "This worker function provided by PyTorch accelerates the training model by pulling images from storage in parallel with the training process, thus speeding up both training and ensuring faster reads from storage.\n",
        "\n",
        "Please note, this cGAN implementation is slightly different than a regular dataloader process since it not only pulls the images, it also tags each 'condition' to the image file.  In the case of this exercise, there are **Four** Conditions.  \n",
        "\n",
        "\n",
        "1.   Neutral Face\n",
        "2.   Happy Face\n",
        "3.   Angry Face\n",
        "4.   Sad Face\n",
        "\n"
      ],
      "metadata": {
        "id": "zZ9m2rn60-nO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Facial Expression Conditions File**\n",
        "\n",
        "The first file to load is a table of file names, along with the condition.  This will be used to both load the file by the dataloader, as well as apply a condition to it's label.\n"
      ],
      "metadata": {
        "id": "wadELFpbCCEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "BlGg-Z5B2FQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generator and Discriminator Class Creation.\n",
        "\n",
        "Continuing with the Tutorial, since the basic theory has been discussed, the Generator will be constructed first.  The model can also be observed once the class has been instantiated.\n",
        "\n",
        "**Generator and Noise**"
      ],
      "metadata": {
        "id": "b0DIuqaRx4_0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nlusk3Z3XlE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExZUcnQHXlHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6wcxOIOXlKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "03qBBPhZXlRG"
      }
    }
  ]
}